{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyP+Kf2/LVAn39Yv29L2DQdQ"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"15iBYz2bqKHh"},"outputs":[],"source":["#@title Import modules\n","import numpy as np\n","import pandas as pd\n","import tensorflow as tf\n","from matplotlib import pyplot as plt\n","\n","pd.options.display.max_rows = 10\n","pd.options.display.float_format = \"{:.1f}\".format"]},{"cell_type":"code","source":["train_df = pd.read_csv(\"https://download.mlcc.google.com/mledu-datasets/california_housing_train.csv\")\n","test_df = pd.read_csv(\"https://download.mlcc.google.com/mledu-datasets/california_housing_test.csv\")"],"metadata":{"id":"7pa9E_e1qMkD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["scale_factor = 1000.0\n","\n","# Scale the training set's label.\n","train_df[\"median_house_value\"] /= scale_factor\n","\n","# Scale the test set's label\n","test_df[\"median_house_value\"] /= scale_factor"],"metadata":{"id":"hK2z2YbzqUR0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#@title Define the functions that build and train a model\n","def build_model(my_learning_rate):\n","  \"\"\"Create and compile a simple linear regression model.\"\"\"\n","  # Most simple tf.keras models are sequential.\n","  model = tf.keras.models.Sequential()\n","\n","  # Add one linear layer to the model to yield a simple linear regressor.\n","  model.add(tf.keras.layers.Dense(units=1, input_shape=(1,)))\n","\n","  # Compile the model topography into code that TensorFlow can efficiently\n","  # execute. Configure training to minimize the model's mean squared error.\n","  model.compile(optimizer=tf.keras.optimizers.experimental.RMSprop(learning_rate=my_learning_rate),\n","                loss=\"mean_squared_error\",\n","                metrics=[tf.keras.metrics.RootMeanSquaredError()])\n","\n","  return model\n","\n","\n","def train_model(model, df, feature, label, my_epochs,\n","                my_batch_size=None, my_validation_split=0.1):\n","  \"\"\"Feed a dataset into the model in order to train it.\"\"\"\n","\n","  history = model.fit(x=df[feature],\n","                      y=df[label],\n","                      batch_size=my_batch_size,\n","                      epochs=my_epochs,\n","                      validation_split=my_validation_split)\n","\n","  # Gather the model's trained weight and bias.\n","  trained_weight = model.get_weights()[0]\n","  trained_bias = model.get_weights()[1]\n","\n","  # The list of epochs is stored separately from the\n","  # rest of history.\n","  epochs = history.epoch\n","\n","  # Isolate the root mean squared error for each epoch.\n","  hist = pd.DataFrame(history.history)\n","  rmse = hist[\"root_mean_squared_error\"]\n","\n","  return epochs, rmse, history.history\n","\n","print(\"Defined the build_model and train_model functions.\")"],"metadata":{"id":"32Kn3RiVqXAQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#@title Define the plotting function\n","\n","def plot_the_loss_curve(epochs, mae_training, mae_validation):\n","  \"\"\"Plot a curve of loss vs. epoch.\"\"\"\n","\n","  plt.figure()\n","  plt.xlabel(\"Epoch\")\n","  plt.ylabel(\"Root Mean Squared Error\")\n","\n","  plt.plot(epochs[1:], mae_training[1:], label=\"Training Loss\")\n","  plt.plot(epochs[1:], mae_validation[1:], label=\"Validation Loss\")\n","  plt.legend()\n","\n","  # We're not going to plot the first epoch, since the loss on the first epoch\n","  # is often substantially greater than the loss for other epochs.\n","  merged_mae_lists = mae_training[1:] + mae_validation[1:]\n","  highest_loss = max(merged_mae_lists)\n","  lowest_loss = min(merged_mae_lists)\n","  delta = highest_loss - lowest_loss\n","  print(delta)\n","\n","  top_of_y_axis = highest_loss + (delta * 0.05)\n","  bottom_of_y_axis = lowest_loss - (delta * 0.05)\n","\n","  plt.ylim([bottom_of_y_axis, top_of_y_axis])\n","  plt.show()\n","\n","print(\"Defined the plot_the_loss_curve function.\")"],"metadata":{"id":"ptA0S13sqaQl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# The following variables are the hyperparameters.\n","learning_rate = 0.08\n","epochs = 30\n","batch_size = 100\n","\n","# Split the original training set into a reduced training set and a\n","# validation set.\n","validation_split = 0.2\n","\n","# Identify the feature and the label.\n","my_feature = \"median_income\"    # the median income on a specific city block.\n","my_label = \"median_house_value\" # the median house value on a specific city block.\n","# That is, you're going to create a model that predicts house value based\n","# solely on the neighborhood's median income.\n","\n","# Invoke the functions to build and train the model.\n","my_model = build_model(learning_rate)\n","epochs, rmse, history = train_model(my_model, train_df, my_feature,\n","                                    my_label, epochs, batch_size,\n","                                    validation_split)\n","\n","plot_the_loss_curve(epochs, history[\"root_mean_squared_error\"],\n","                    history[\"val_root_mean_squared_error\"])"],"metadata":{"id":"ohx9Rj8xqdG3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Examine examples 0 through 4 and examples 995 through 999\n","# of the training set\n","train_df.head(n=1000)\n","\n","# The original training set is sorted by longitude.\n","# Apparently, longitude influences the relationship of\n","# total_rooms to median_house_value."],"metadata":{"id":"laQeQu2Nqf3K"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# The following variables are the hyperparameters.\n","learning_rate = 0.08\n","epochs = 70\n","batch_size = 100\n","\n","# Split the original training set into a reduced training set and a\n","# validation set.\n","validation_split = 0.2\n","\n","# Identify the feature and the label.\n","my_feature = \"median_income\"    # the median income on a specific city block.\n","my_label = \"median_house_value\" # the median house value on a specific city block.\n","# That is, you're going to create a model that predicts house value based\n","# solely on the neighborhood's median income.\n","\n","# Shuffle the examples.\n","shuffled_train_df = train_df.reindex(np.random.permutation(train_df.index))\n","\n","# Invoke the functions to build and train the model. Train on the shuffled\n","# training set.\n","my_model = build_model(learning_rate)\n","epochs, rmse, history = train_model(my_model, shuffled_train_df, my_feature,\n","                                    my_label, epochs, batch_size,\n","                                    validation_split)\n","\n","plot_the_loss_curve(epochs, history[\"root_mean_squared_error\"],\n","                    history[\"val_root_mean_squared_error\"])"],"metadata":{"id":"Fp8LiyKzqjxc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# After shuffling the original training set,\n","# the final loss for the training set and the\n","# validation set become much closer.\n","\n","# If validation_split < 0.15,\n","# the final loss values for the training set and\n","# validation set diverge meaningfully.  Apparently,\n","# the validation set no longer contains enough examples."],"metadata":{"id":"CzEX4IFmqn4u"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["x_test = test_df[my_feature]\n","y_test = test_df[my_label]\n","\n","results = my_model.evaluate(x_test, y_test, batch_size=batch_size)"],"metadata":{"id":"wToJfgQbqtGU"},"execution_count":null,"outputs":[]}]}